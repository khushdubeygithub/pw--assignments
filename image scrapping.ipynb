{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9b9408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "                                              0.0/62.6 kB ? eta -:--:--\n",
      "     -------------------                      30.7/62.6 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  61.4/62.6 kB 656.4 kB/s eta 0:00:01\n",
      "     -------------------------------------- 62.6/62.6 kB 556.7 kB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.2.0-cp311-cp311-win_amd64.whl (96 kB)\n",
      "                                              0.0/96.6 kB ? eta -:--:--\n",
      "     ------------                             30.7/96.6 kB 1.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 96.6/96.6 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\khushi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "                                              0.0/123.9 kB ? eta -:--:--\n",
      "                                              0.0/123.9 kB ? eta -:--:--\n",
      "                                              0.0/123.9 kB ? eta -:--:--\n",
      "     -------------                            41.0/123.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 123.9/123.9 kB 3.7 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "                                              0.0/158.3 kB ? eta -:--:--\n",
      "     ----------                              41.0/158.3 kB 1.9 MB/s eta 0:00:01\n",
      "     ----------------------                  92.2/158.3 kB 1.3 MB/s eta 0:00:01\n",
      "     ----------------------------------     143.4/158.3 kB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ 158.3/158.3 kB 949.3 kB/s eta 0:00:00\n",
      "Installing collected packages: urllib3, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2023.7.22 charset-normalizer-3.2.0 requests-2.31.0 urllib3-2.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94ac125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_video_urls(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    video_urls = []\n",
    "    for video in soup.find_all('div', class_='ytd-video-renderer'):\n",
    "        video_url = video.find('a', class_='ytd-video-renderer-thumbnail')['href']\n",
    "        video_urls.append(video_url)\n",
    "    return video_urls\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "    video_urls = get_video_urls(url)\n",
    "    for video_url in video_urls[:5]:\n",
    "        print(video_url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642a80b9",
   "metadata": {},
   "source": [
    " q2 Write a python program to extract the URL of the video thumbnails of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab95ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_video_thumbnail_urls(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    thumbnail_urls = []\n",
    "    for video in soup.find_all('div', class_='ytd-video-renderer'):\n",
    "        thumbnail_url = video.find('img', class_='ytd-thumbnail')['src']\n",
    "        thumbnail_urls.append(thumbnail_url)\n",
    "    return thumbnail_urls\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "    thumbnail_urls = get_video_thumbnail_urls(url)\n",
    "    for thumbnail_url in thumbnail_urls[:5]:\n",
    "        print(thumbnail_url)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e92ebef",
   "metadata": {},
   "source": [
    "Q3. Write a python program to extract the title of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579e5554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_video_titles(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    titles = []\n",
    "    for video in soup.find_all('div', class_='ytd-video-renderer'):\n",
    "        title = video.find('a', class_='ytd-video-renderer-title').text\n",
    "        titles.append(title)\n",
    "    return titles\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "    titles = get_video_titles(url)\n",
    "    for title in titles[:5]:\n",
    "        print(title)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c8fc4",
   "metadata": {},
   "source": [
    "Q4. Write a python program to extract the number of views of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25a12585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_video_views(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    views = []\n",
    "    for video in soup.find_all('div', class_='ytd-video-renderer'):\n",
    "        view_count = video.find('span', class_='view-count').text\n",
    "        view_count = view_count.replace(' views', '')\n",
    "        views.append(view_count)\n",
    "    return views\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "    views = get_video_views(url)\n",
    "    for view in views[:5]:\n",
    "        print(view)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed78c977",
   "metadata": {},
   "source": [
    "Q5. Write a python program to extract the time of posting of video for the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab9a6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_video_published_time(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    published_times = []\n",
    "    for video in soup.find_all('div', class_='ytd-video-renderer'):\n",
    "        published_time = video.find('time', class_='ytd-video-renderer-date')['datetime']\n",
    "        published_times.append(published_time)\n",
    "    return published_times\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "    published_times = get_video_published_time(url)\n",
    "    for published_time in published_times[:5]:\n",
    "        print(published_time)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "808049d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all the data scraped in the above questions in a CSV file.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def get_video_data(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    data = []\n",
    "    for video in soup.find_all('div', class_='ytd-video-renderer'):\n",
    "        video_url = video.find('a', class_='ytd-video-renderer-thumbnail')['href']\n",
    "        thumbnail_url = video.find('img', class_='ytd-thumbnail')['src']\n",
    "        title = video.find('a', class_='ytd-video-renderer-title').text\n",
    "        view_count = video.find('span', class_='view-count').text.replace(' views', '')\n",
    "        published_time = video.find('time', class_='ytd-video-renderer-date')['datetime']\n",
    "        data.append((video_url, thumbnail_url, title, view_count, published_time))\n",
    "    return data\n",
    "\n",
    "def save_data_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=',')\n",
    "        writer.writerow(['video_url', 'thumbnail_url', 'title', 'view_count', 'published_time'])\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "    data = get_video_data(url)\n",
    "    filename = 'pw_foundation_videos.csv'\n",
    "    save_data_to_csv(data, filename)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0a9c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
